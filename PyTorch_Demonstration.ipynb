{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#This is my notes on the example that was used in the week 9 lecture.\n",
        "#This Google Collab file is linked with my GitHub. It is unrelated to the project but is useful\n",
        "#to refer back to to get some frame of reference when developing my project."
      ],
      "metadata": {
        "id": "Q3gp-0uzWoUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Preprocessing the Iris Dataset**"
      ],
      "metadata": {
        "id": "v0ggsakHcARk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pk2aK_zV52S",
        "outputId": "4517e22c-9395-4c38-d953-e010fce6ee81"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
            "0                5.1               3.5                1.4               0.2\n",
            "1                4.9               3.0                1.4               0.2\n",
            "2                4.7               3.2                1.3               0.2\n",
            "3                4.6               3.1                1.5               0.2\n",
            "4                5.0               3.6                1.4               0.2\n",
            "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
            "145                6.7               3.0                5.2               2.3\n",
            "146                6.3               2.5                5.0               1.9\n",
            "147                6.5               3.0                5.2               2.0\n",
            "148                6.2               3.4                5.4               2.3\n",
            "149                5.9               3.0                5.1               1.8\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn import datasets\n",
        "\n",
        "#Load the iris dataset from sklearn (we'll use a similar one in the assessment)\n",
        "#Contains basic information about a bunch of flowers\n",
        "iris = datasets.load_iris()\n",
        "df = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "print(df.head())\n",
        "print(df.tail())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Target is the our data labels, i.e. training data - we set it to float data type of later learning algorithms\n",
        "#The values 0-2 indicate the three types of flower in the dataset\n",
        "df[\"target\"] = iris.target\n",
        "df[\"target\"] = df[\"target\"].astype(float)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a95e0DF7YM8k",
        "outputId": "a3632cf1-7497-40ea-aad4-0483b8822888"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
            "0                  5.1               3.5                1.4               0.2   \n",
            "1                  4.9               3.0                1.4               0.2   \n",
            "2                  4.7               3.2                1.3               0.2   \n",
            "3                  4.6               3.1                1.5               0.2   \n",
            "4                  5.0               3.6                1.4               0.2   \n",
            "..                 ...               ...                ...               ...   \n",
            "145                6.7               3.0                5.2               2.3   \n",
            "146                6.3               2.5                5.0               1.9   \n",
            "147                6.5               3.0                5.2               2.0   \n",
            "148                6.2               3.4                5.4               2.3   \n",
            "149                5.9               3.0                5.1               1.8   \n",
            "\n",
            "     target  \n",
            "0       0.0  \n",
            "1       0.0  \n",
            "2       0.0  \n",
            "3       0.0  \n",
            "4       0.0  \n",
            "..      ...  \n",
            "145     2.0  \n",
            "146     2.0  \n",
            "147     2.0  \n",
            "148     2.0  \n",
            "149     2.0  \n",
            "\n",
            "[150 rows x 5 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Now we are going to preprocess the data\n",
        "x = df.drop(\"target\", axis=1).values  #Dataset\n",
        "y = df[\"target\"].values   #Labels\n",
        "print(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AH74rCkvafBO",
        "outputId": "d0e8af78-0f2b-4e31-acaa-3282779b1433"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]\n",
            " [5.4 3.9 1.7 0.4]\n",
            " [4.6 3.4 1.4 0.3]\n",
            " [5.  3.4 1.5 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.4 3.7 1.5 0.2]\n",
            " [4.8 3.4 1.6 0.2]\n",
            " [4.8 3.  1.4 0.1]\n",
            " [4.3 3.  1.1 0.1]\n",
            " [5.8 4.  1.2 0.2]\n",
            " [5.7 4.4 1.5 0.4]\n",
            " [5.4 3.9 1.3 0.4]\n",
            " [5.1 3.5 1.4 0.3]\n",
            " [5.7 3.8 1.7 0.3]\n",
            " [5.1 3.8 1.5 0.3]\n",
            " [5.4 3.4 1.7 0.2]\n",
            " [5.1 3.7 1.5 0.4]\n",
            " [4.6 3.6 1.  0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [5.  3.  1.6 0.2]\n",
            " [5.  3.4 1.6 0.4]\n",
            " [5.2 3.5 1.5 0.2]\n",
            " [5.2 3.4 1.4 0.2]\n",
            " [4.7 3.2 1.6 0.2]\n",
            " [4.8 3.1 1.6 0.2]\n",
            " [5.4 3.4 1.5 0.4]\n",
            " [5.2 4.1 1.5 0.1]\n",
            " [5.5 4.2 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.2]\n",
            " [5.  3.2 1.2 0.2]\n",
            " [5.5 3.5 1.3 0.2]\n",
            " [4.9 3.6 1.4 0.1]\n",
            " [4.4 3.  1.3 0.2]\n",
            " [5.1 3.4 1.5 0.2]\n",
            " [5.  3.5 1.3 0.3]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [4.4 3.2 1.3 0.2]\n",
            " [5.  3.5 1.6 0.6]\n",
            " [5.1 3.8 1.9 0.4]\n",
            " [4.8 3.  1.4 0.3]\n",
            " [5.1 3.8 1.6 0.2]\n",
            " [4.6 3.2 1.4 0.2]\n",
            " [5.3 3.7 1.5 0.2]\n",
            " [5.  3.3 1.4 0.2]\n",
            " [7.  3.2 4.7 1.4]\n",
            " [6.4 3.2 4.5 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [5.5 2.3 4.  1.3]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [5.7 2.8 4.5 1.3]\n",
            " [6.3 3.3 4.7 1.6]\n",
            " [4.9 2.4 3.3 1. ]\n",
            " [6.6 2.9 4.6 1.3]\n",
            " [5.2 2.7 3.9 1.4]\n",
            " [5.  2.  3.5 1. ]\n",
            " [5.9 3.  4.2 1.5]\n",
            " [6.  2.2 4.  1. ]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [5.6 2.9 3.6 1.3]\n",
            " [6.7 3.1 4.4 1.4]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.8 2.7 4.1 1. ]\n",
            " [6.2 2.2 4.5 1.5]\n",
            " [5.6 2.5 3.9 1.1]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [6.1 2.8 4.  1.3]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.8 4.7 1.2]\n",
            " [6.4 2.9 4.3 1.3]\n",
            " [6.6 3.  4.4 1.4]\n",
            " [6.8 2.8 4.8 1.4]\n",
            " [6.7 3.  5.  1.7]\n",
            " [6.  2.9 4.5 1.5]\n",
            " [5.7 2.6 3.5 1. ]\n",
            " [5.5 2.4 3.8 1.1]\n",
            " [5.5 2.4 3.7 1. ]\n",
            " [5.8 2.7 3.9 1.2]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.  3.4 4.5 1.6]\n",
            " [6.7 3.1 4.7 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [5.6 3.  4.1 1.3]\n",
            " [5.5 2.5 4.  1.3]\n",
            " [5.5 2.6 4.4 1.2]\n",
            " [6.1 3.  4.6 1.4]\n",
            " [5.8 2.6 4.  1.2]\n",
            " [5.  2.3 3.3 1. ]\n",
            " [5.6 2.7 4.2 1.3]\n",
            " [5.7 3.  4.2 1.2]\n",
            " [5.7 2.9 4.2 1.3]\n",
            " [6.2 2.9 4.3 1.3]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [5.7 2.8 4.1 1.3]\n",
            " [6.3 3.3 6.  2.5]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [7.1 3.  5.9 2.1]\n",
            " [6.3 2.9 5.6 1.8]\n",
            " [6.5 3.  5.8 2.2]\n",
            " [7.6 3.  6.6 2.1]\n",
            " [4.9 2.5 4.5 1.7]\n",
            " [7.3 2.9 6.3 1.8]\n",
            " [6.7 2.5 5.8 1.8]\n",
            " [7.2 3.6 6.1 2.5]\n",
            " [6.5 3.2 5.1 2. ]\n",
            " [6.4 2.7 5.3 1.9]\n",
            " [6.8 3.  5.5 2.1]\n",
            " [5.7 2.5 5.  2. ]\n",
            " [5.8 2.8 5.1 2.4]\n",
            " [6.4 3.2 5.3 2.3]\n",
            " [6.5 3.  5.5 1.8]\n",
            " [7.7 3.8 6.7 2.2]\n",
            " [7.7 2.6 6.9 2.3]\n",
            " [6.  2.2 5.  1.5]\n",
            " [6.9 3.2 5.7 2.3]\n",
            " [5.6 2.8 4.9 2. ]\n",
            " [7.7 2.8 6.7 2. ]\n",
            " [6.3 2.7 4.9 1.8]\n",
            " [6.7 3.3 5.7 2.1]\n",
            " [7.2 3.2 6.  1.8]\n",
            " [6.2 2.8 4.8 1.8]\n",
            " [6.1 3.  4.9 1.8]\n",
            " [6.4 2.8 5.6 2.1]\n",
            " [7.2 3.  5.8 1.6]\n",
            " [7.4 2.8 6.1 1.9]\n",
            " [7.9 3.8 6.4 2. ]\n",
            " [6.4 2.8 5.6 2.2]\n",
            " [6.3 2.8 5.1 1.5]\n",
            " [6.1 2.6 5.6 1.4]\n",
            " [7.7 3.  6.1 2.3]\n",
            " [6.3 3.4 5.6 2.4]\n",
            " [6.4 3.1 5.5 1.8]\n",
            " [6.  3.  4.8 1.8]\n",
            " [6.9 3.1 5.4 2.1]\n",
            " [6.7 3.1 5.6 2.4]\n",
            " [6.9 3.1 5.1 2.3]\n",
            " [5.8 2.7 5.1 1.9]\n",
            " [6.8 3.2 5.9 2.3]\n",
            " [6.7 3.3 5.7 2.5]\n",
            " [6.7 3.  5.2 2.3]\n",
            " [6.3 2.5 5.  1.9]\n",
            " [6.5 3.  5.2 2. ]\n",
            " [6.2 3.4 5.4 2.3]\n",
            " [5.9 3.  5.1 1.8]]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
            " 1. 1. 1. 1. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2. 2.\n",
            " 2. 2. 2. 2. 2. 2.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural Networks**"
      ],
      "metadata": {
        "id": "sK0M1d-sbvxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Time to start building the neural network - import PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F   #This contains useful stuff like the loss and activation functions"
      ],
      "metadata": {
        "id": "lr9RXN0eaL1f"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class My_NN(nn.Module): #PyTorch neural network module - defines the class as a neural network\n",
        "  def __init__(self, in_feature, hidden_layers, out_features, activation_function = F.relu)   #in_feature - number of input features (in this case 4, sepal and petal width/length)\n",
        "                                                                                              #hidden_layers - list of integers, indicates the number of neurones on each layer\n",
        "                                                                                              #out_features - number of output features (in this case 1, a float of value 0-2)\n",
        "                                                                                              #activation_function - we're using relu to train the model (over sigmoid/step)\n",
        "    super().__init__()\n",
        "    if len(hidden_layers) < 1:\n",
        "      raise Exception(\"A My_NN must have at least 1 hiddel layer\")  #(Because it does)\n",
        "    #Now let's add neurones by layer\n",
        "    self.layers = []\n",
        "    self.layers.append(nn.Linear(in_features, hidden_layers[0]))  #Takes a number of input features defined earlier, produces a number of outputs (equal to whatever is in the first element of hidden_layers)\n",
        "    self.add_module(\"input_layer\", self.layers[0]) #We name this first layer as the input layer\n",
        "    for i in range(1, len(hidden_layers)-1) :\n",
        "      self.layers.append(nn.Linear(hidden_layers[i-1], hidden_layers[i])) #Repeat for all layers - #input features = number of output features of last layer\n",
        "      self.add_module(\"hidden_layer_\" + str(self.layers[i]), self.layer[1]) #All intermediary layers are named hidden layers\n",
        "    self.layers.append(nn.Linear(hidden_layers[len(hidden_layers-1)], out_features))  #Add the last layer - produces a number of output equal to whatever was defined earlier\n",
        "    self.add_module(\"output_layer\", self.layer[len(self.layers-1)]) #The last layer is named the output layer\n",
        "\n",
        "    #We can get the output layer instead of the entire neural network\n",
        "    self.out = self.layers[len(self.layers)-1]\n",
        "    self.activation_function = activation_function\n",
        "\n",
        "  def forward(self, x): #Feed Forward Network - Forwards the output values to the next layer\n",
        "    for i in range(len(self.layers)):\n",
        ""
      ],
      "metadata": {
        "id": "9LQcJ5ducO6c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}